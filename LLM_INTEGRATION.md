# å¤§æ¨¡å‹é›†æˆæŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨çœŸå®çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€Claudeã€DeepSeek ç­‰ï¼‰é€šè¿‡ SWE-agent æˆ–å…¶ä»–æ¡†æ¶æ¥æµ‹è¯• PDEBenchã€‚

## æ–¹æ¡ˆ 1: ä½¿ç”¨ SWE-agentï¼ˆæ¨èï¼‰

### 1.1 å®‰è£… SWE-agent

```bash
# åœ¨ pdebench ç›®å½•å¤–å…‹éš† SWE-agent
cd /Users/yusan/agent
git clone https://github.com/princeton-nlp/SWE-agent.git
cd SWE-agent
pip install -e .
```

### 1.2 åˆ›å»º PDEBench ä»»åŠ¡é€‚é…å™¨

ä¸º SWE-agent åˆ›å»ºä»»åŠ¡æè¿°æ–‡ä»¶ï¼š

```bash
cd /Users/yusan/agent/pdebench
python scripts/create_swe_tasks.py --dataset datasets/level_2_1_basic.jsonl --output swe_tasks/
```

è¿™å°†ç”Ÿæˆç»“æ„å¦‚ä¸‹çš„ä»»åŠ¡æ–‡ä»¶ï¼š

```json
{
  "instance_id": "pdebench__poisson_simple",
  "repo": "pdebench",
  "base_commit": "main",
  "problem_statement": "Solve the Poisson equation...",
  "hints_text": "",
  "test_patch": "",
  "version": "1.0"
}
```

### 1.3 é…ç½® LLM API

```bash
# è®¾ç½® API å¯†é’¥
export OPENAI_API_KEY="your-key-here"
export ANTHROPIC_API_KEY="your-key-here"
export DEEPSEEK_API_KEY="your-key-here"
```

### 1.4 è¿è¡Œ SWE-agent

```bash
cd /Users/yusan/agent/SWE-agent

# ä½¿ç”¨ GPT-4
python run.py \
  --model_name gpt4 \
  --data_path /Users/yusan/agent/pdebench/swe_tasks/poisson_simple.json \
  --config_file config/default.yaml \
  --output_dir /Users/yusan/agent/pdebench/results/swe_gpt4

# ä½¿ç”¨ Claude
python run.py \
  --model_name claude-3-opus-20240229 \
  --data_path /Users/yusan/agent/pdebench/swe_tasks/poisson_simple.json \
  --output_dir /Users/yusan/agent/pdebench/results/swe_claude
```

### 1.5 æå– Agent ç”Ÿæˆçš„ä»£ç 

SWE-agent ä¼šç”Ÿæˆå®Œæ•´çš„ä»£ç ä¿®æ”¹ï¼Œæˆ‘ä»¬éœ€è¦æå–ç”Ÿæˆçš„æ±‚è§£å™¨è„šæœ¬ï¼š

```bash
python scripts/extract_swe_output.py \
  --swe-output results/swe_gpt4/trajectories/poisson_simple \
  --output-script generated_solvers/gpt4_poisson_simple.py
```

### 1.6 è¯„ä¼°ç”Ÿæˆçš„ä»£ç 

```bash
python scripts/evaluate_agent.py \
  --dataset datasets/level_2_1_basic.jsonl \
  --agent-script generated_solvers/gpt4_poisson_simple.py \
  --outdir results/eval_gpt4
```

## æ–¹æ¡ˆ 2: ç›´æ¥ API è°ƒç”¨ï¼ˆæ›´çµæ´»ï¼‰

### 2.1 åˆ›å»º LLM è°ƒç”¨åŒ…è£…å™¨

```bash
cd /Users/yusan/agent/pdebench
python scripts/run_llm_benchmark.py \
  --dataset datasets/level_2_1_basic.jsonl \
  --model gpt-4 \
  --provider openai \
  --outdir results/llm_gpt4_direct \
  --limit 5
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. ä»æ•°æ®é›†è¯»å–æ¯ä¸ªé—®é¢˜
2. æ„é€  prompt å‘é€ç»™ LLM
3. è§£æ LLM è¿”å›çš„ä»£ç 
4. è‡ªåŠ¨æ‰§è¡Œå’Œè¯„ä¼°

### 2.2 é…ç½®æ–‡ä»¶ç¤ºä¾‹

åˆ›å»º `configs/llm_providers.yaml`ï¼š

```yaml
providers:
  openai:
    api_key_env: OPENAI_API_KEY
    models:
      - gpt-4
      - gpt-4-turbo
      - gpt-3.5-turbo
    
  anthropic:
    api_key_env: ANTHROPIC_API_KEY
    models:
      - claude-3-opus-20240229
      - claude-3-sonnet-20240229
  
  deepseek:
    api_key_env: DEEPSEEK_API_KEY
    base_url: https://api.deepseek.com/v1
    models:
      - deepseek-coder

prompt_template: |
  You are an expert computational physicist. Your task is to implement a finite element solver using FEniCSx (dolfinx).
  
  {problem_statement}
  
  Please generate a complete, runnable Python script that:
  {requirements}
  
  Output only the Python code, enclosed in ```python code blocks.
```

## æ–¹æ¡ˆ 3: æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¨¡å‹

### 3.1 åˆ›å»ºæ‰¹é‡è¯„ä¼°è„šæœ¬

```bash
python scripts/batch_evaluate_llms.py \
  --dataset datasets/level_2_1_basic.jsonl \
  --models gpt-4 claude-3-opus deepseek-coder \
  --outdir results/multi_model_comparison \
  --runs-per-model 3
```

### 3.2 æŸ¥çœ‹å¯¹æ¯”ç»“æœ

```bash
python scripts/compare_results.py \
  --result-dirs results/llm_gpt4 results/llm_claude results/llm_deepseek \
  --output results/comparison_report.html
```

ç”Ÿæˆçš„æŠ¥å‘ŠåŒ…å«ï¼š
- å„æ¨¡å‹æˆåŠŸç‡å¯¹æ¯”
- ç²¾åº¦åˆ†å¸ƒå›¾
- è¿è¡Œæ—¶é—´åˆ†æ
- ä»£ç è´¨é‡è¯„ä¼°

## å®ç°ç»†èŠ‚

### åˆ›å»º SWE-agent ä»»åŠ¡ç”Ÿæˆå™¨

åˆ›å»º `scripts/create_swe_tasks.py`ï¼š

```python
#!/usr/bin/env python3
"""Convert PDEBench dataset to SWE-agent task format."""

import argparse
import json
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))
from pdebench.datasets.schema import load_dataset


def create_swe_task(entry, repo_path):
    """Convert a dataset entry to SWE-agent task format."""
    
    # æ„é€ é—®é¢˜é™ˆè¿°
    problem_statement = f"""# PDEBench Task: {entry.id}

{entry.prompt}

## Technical Requirements
{chr(10).join(f"- {req}" for req in entry.requirements)}

## Expected Output Files
1. `agent_solver.py` - Your implementation
2. The script should accept: --resolution N --degree P --outdir DIR
3. Output files: solution.npz and meta.json

## Workspace Setup
Your code will be tested in an isolated environment with:
- Python 3.10
- FEniCSx (dolfinx) installed
- PETSc and MPI available

## Validation
Your solution will be compared against a reference solution using relative L2 error.
Target: {entry.evaluation_config.get('target_metric')} â‰¤ {entry.evaluation_config.get('target_error')}
"""
    
    return {
        "instance_id": f"pdebench__{entry.id}",
        "repo": str(repo_path.absolute()),
        "base_commit": "HEAD",
        "problem_statement": problem_statement,
        "hints_text": "",
        "created_at": "2024-12-17",
        "version": "1.0",
        "FAIL_TO_PASS": ["test_solution_accuracy"],
        "PASS_TO_PASS": [],
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=Path, required=True)
    parser.add_argument('--output', type=Path, required=True)
    parser.add_argument('--repo-path', type=Path, default=Path.cwd())
    args = parser.parse_args()
    
    # Load dataset
    entries = load_dataset(str(args.dataset))
    
    # Create output directory
    args.output.mkdir(parents=True, exist_ok=True)
    
    # Convert each entry
    for entry in entries:
        task = create_swe_task(entry, args.repo_path)
        
        output_file = args.output / f"{entry.id}.json"
        with open(output_file, 'w') as f:
            json.dump(task, f, indent=2)
        
        print(f"âœ“ Created: {output_file}")
    
    print(f"\nâœ… Generated {len(entries)} SWE-agent tasks")


if __name__ == '__main__':
    main()
```

### åˆ›å»ºç›´æ¥ LLM è°ƒç”¨è„šæœ¬

åˆ›å»º `scripts/run_llm_benchmark.py`ï¼š

```python
#!/usr/bin/env python3
"""Run LLM benchmark with direct API calls."""

import argparse
import json
import os
import re
import time
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from pdebench.datasets.schema import load_dataset
from pdebench.sandbox.executor import execute_agent_script_with_oracle
from pdebench.evaluation.validator import validate_solution


def call_llm(prompt, model, provider):
    """Call LLM API and return generated code."""
    
    if provider == 'openai':
        import openai
        openai.api_key = os.getenv('OPENAI_API_KEY')
        
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert in computational physics and finite element methods."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=4000
        )
        
        return response.choices[0].message.content
    
    elif provider == 'anthropic':
        import anthropic
        client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        
        response = client.messages.create(
            model=model,
            max_tokens=4000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.content[0].text
    
    else:
        raise ValueError(f"Unknown provider: {provider}")


def extract_code(llm_response):
    """Extract Python code from LLM response."""
    # Find code blocks
    pattern = r'```python\n(.*?)```'
    matches = re.findall(pattern, llm_response, re.DOTALL)
    
    if matches:
        return matches[0]
    
    # Fallback: return entire response
    return llm_response


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=Path, required=True)
    parser.add_argument('--model', type=str, required=True)
    parser.add_argument('--provider', choices=['openai', 'anthropic'], required=True)
    parser.add_argument('--outdir', type=Path, required=True)
    parser.add_argument('--limit', type=int)
    args = parser.parse_args()
    
    # Load dataset
    entries = load_dataset(str(args.dataset))
    if args.limit:
        entries = entries[:args.limit]
    
    args.outdir.mkdir(parents=True, exist_ok=True)
    
    results = []
    
    for i, entry in enumerate(entries, 1):
        print(f"\n[{i}/{len(entries)}] Processing: {entry.id}")
        
        case_dir = args.outdir / entry.id
        case_dir.mkdir(parents=True, exist_ok=True)
        
        # Call LLM
        print("  ğŸ“¡ Calling LLM...")
        try:
            llm_response = call_llm(entry.prompt, args.model, args.provider)
            code = extract_code(llm_response)
            
            # Save generated code
            script_path = case_dir / 'generated_solver.py'
            with open(script_path, 'w') as f:
                f.write(code)
            
            # Save full response
            with open(case_dir / 'llm_response.txt', 'w') as f:
                f.write(llm_response)
            
            print(f"  âœ“ Generated code ({len(code)} chars)")
            
            # Execute and evaluate
            print("  ğŸ”§ Executing...")
            exec_result, agent_out, oracle_out = execute_agent_script_with_oracle(
                script_path=script_path,
                oracle_config=entry.oracle_config,
                base_outdir=case_dir,
                evaluation_config=entry.evaluation_config
            )
            
            if exec_result.success:
                print("  âœ“ Execution successful")
                
                validation = validate_solution(agent_out, oracle_out, entry.evaluation_config)
                print(f"  {'âœ“' if validation.is_valid else 'âœ—'} {validation.reason}")
                
                result = {
                    'case_id': entry.id,
                    'success': validation.is_valid,
                    'execution': exec_result.to_dict(),
                    'validation': validation.to_dict()
                }
            else:
                print(f"  âœ— Execution failed: {exec_result.error_message}")
                result = {
                    'case_id': entry.id,
                    'success': False,
                    'execution': exec_result.to_dict(),
                    'validation': None
                }
            
        except Exception as e:
            print(f"  âœ— Error: {str(e)}")
            result = {
                'case_id': entry.id,
                'success': False,
                'error': str(e)
            }
        
        results.append(result)
        
        # Save intermediate results
        with open(case_dir / 'result.json', 'w') as f:
            json.dump(result, f, indent=2)
        
        # Rate limiting
        time.sleep(1)
    
    # Generate summary
    summary = {
        'model': args.model,
        'provider': args.provider,
        'total_cases': len(results),
        'successful': sum(1 for r in results if r.get('success', False)),
        'results': results
    }
    
    with open(args.outdir / 'summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nâœ… Completed: {summary['successful']}/{summary['total_cases']} passed")


if __name__ == '__main__':
    main()
```

## è¿è¡Œç¤ºä¾‹

```bash
# 1. ç”Ÿæˆ SWE-agent ä»»åŠ¡
python scripts/create_swe_tasks.py \
  --dataset datasets/level_2_1_basic.jsonl \
  --output swe_tasks/

# 2. ç›´æ¥ API è°ƒç”¨æµ‹è¯•
export OPENAI_API_KEY="sk-..."
python scripts/run_llm_benchmark.py \
  --dataset datasets/level_2_1_basic.jsonl \
  --model gpt-4 \
  --provider openai \
  --outdir results/gpt4_test \
  --limit 2

# 3. æŸ¥çœ‹ç»“æœ
cat results/gpt4_test/summary.json | jq '.successful, .total_cases'
```

## é¢„æœŸç»“æœ

ä¸åŒæ¨¡å‹çš„é¢„æœŸè¡¨ç°ï¼š

| æ¨¡å‹ | Level 2.1 é¢„æœŸé€šè¿‡ç‡ | Level 2.2 é¢„æœŸé€šè¿‡ç‡ |
|------|---------------------|---------------------|
| GPT-4 | 70-90% | 30-50% |
| Claude-3-Opus | 60-80% | 20-40% |
| GPT-3.5 | 30-50% | 5-15% |
| DeepSeek-Coder | 40-60% | 10-25% |

Level 2.2 éš¾åº¦è¾ƒé«˜ï¼Œå› ä¸ºéœ€è¦è¯†åˆ«æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼ˆå¦‚é«˜ PÃ©clet æ•°éœ€è¦ SUPGï¼‰ã€‚

