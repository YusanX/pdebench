#!/usr/bin/env python3
"""Evaluate agent performance on the benchmark dataset.

Complete evaluation pipeline:
1. Load dataset entries
2. For each entry:
   a. Execute agent script in sandbox
   b. Generate oracle ground truth
   c. Validate solution against oracle
   d. Compute metrics
3. Generate summary report

Usage:
    python scripts/evaluate_agent.py --dataset datasets/level_2_1_basic.jsonl \
                                      --agent-script path/to/agent_solver.py \
                                      --outdir results/run_001
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import Dict, Any, List

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from pdebench.datasets.schema import load_dataset
from pdebench.sandbox.executor import execute_agent_script_with_oracle
from pdebench.evaluation.validator import validate_solution


def evaluate_single_case(
    case_id: str,
    agent_script: Path,
    oracle_config: Dict[str, Any],
    evaluation_config: Dict[str, Any],
    output_dir: Path
) -> Dict[str, Any]:
    """
    Evaluate agent on a single benchmark case.
    
    Args:
        case_id: Case identifier
        agent_script: Path to agent script
        oracle_config: Oracle configuration (case spec)
        evaluation_config: Evaluation parameters
        output_dir: Output directory for this case
    
    Returns:
        Dictionary with evaluation results
    """
    print(f"\n{'='*60}")
    print(f"Evaluating: {case_id}")
    print(f"{'='*60}")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    t_start = time.time()
    
    # Execute agent script and generate oracle
    try:
        exec_result, agent_outdir, oracle_outdir = execute_agent_script_with_oracle(
            script_path=agent_script,
            oracle_config=oracle_config,
            base_outdir=output_dir,
            evaluation_config=evaluation_config
        )
        
        print(f"  Agent execution: {'✓ Success' if exec_result.success else '✗ Failed'}")
        print(f"  Wall time: {exec_result.wall_time_sec:.2f}s")
        
        if not exec_result.success:
            print(f"  Error: {exec_result.error_message}")
            
            return {
                'case_id': case_id,
                'success': False,
                'execution': exec_result.to_dict(),
                'validation': None,
                'total_time_sec': time.time() - t_start,
            }
        
        # Validate solution
        validation_result = validate_solution(
            agent_outdir=agent_outdir,
            oracle_outdir=oracle_outdir,
            evaluation_config=evaluation_config
        )
        
        print(f"  Validation: {'✓ Pass' if validation_result.is_valid else '✗ Fail'}")
        print(f"  {validation_result.reason}")
        
        result = {
            'case_id': case_id,
            'success': validation_result.is_valid,
            'execution': exec_result.to_dict(),
            'validation': validation_result.to_dict(),
            'total_time_sec': time.time() - t_start,
        }
        
        # Save case result
        with open(output_dir / 'result.json', 'w') as f:
            json.dump(result, f, indent=2)
        
        return result
    
    except Exception as e:
        print(f"  ✗ Exception: {str(e)}")
        
        return {
            'case_id': case_id,
            'success': False,
            'execution': None,
            'validation': None,
            'error': str(e),
            'total_time_sec': time.time() - t_start,
        }


def generate_mock_agent_script(
    entry_id: str,
    oracle_config: Dict[str, Any],
    output_path: Path
):
    """
    Generate a mock agent script that uses the oracle solver.
    
    This is useful for testing the evaluation pipeline without a real agent.
    
    Args:
        entry_id: Dataset entry ID
        oracle_config: Oracle configuration
        output_path: Path to save mock script
    """
    # Convert config to Python repr format (not JSON)
    import pprint
    config_str = pprint.pformat(oracle_config, indent=2, width=100)
    
    script_content = f'''#!/usr/bin/env python3
"""Mock agent script for case: {entry_id}

This script uses the Oracle solver to verify the evaluation pipeline.
In a real scenario, this would be generated by an AI agent.
"""

import argparse
import sys
from pathlib import Path

# Import oracle solver
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from pdebench.oracle import generate, solve_case

# Oracle configuration (hidden from real agent)
ORACLE_CONFIG = {config_str}


def main():
    parser = argparse.ArgumentParser(description='PDE solver')
    parser.add_argument('--resolution', type=int, required=True)
    parser.add_argument('--degree', type=int, required=True)
    parser.add_argument('--outdir', type=str, required=True)
    args = parser.parse_args()
    
    outdir = Path(args.outdir)
    
    # Update config with provided parameters
    config = ORACLE_CONFIG.copy()
    config['mesh']['resolution'] = args.resolution
    config['fem']['degree'] = args.degree
    
    # Use oracle solver (this simulates perfect agent performance)
    generate(config, outdir)
    solve_case(config, outdir)


if __name__ == '__main__':
    main()
'''
    
    with open(output_path, 'w') as f:
        f.write(script_content)
    
    output_path.chmod(0o755)  # Make executable


def generate_summary_report(
    results: List[Dict[str, Any]],
    output_file: Path
):
    """
    Generate summary report from evaluation results.
    
    Args:
        results: List of case evaluation results
        output_file: Path to save report
    """
    total_cases = len(results)
    successful_cases = sum(1 for r in results if r.get('success', False))
    failed_cases = total_cases - successful_cases
    
    success_rate = successful_cases / total_cases if total_cases > 0 else 0.0
    
    # Compute statistics
    valid_results = [r for r in results if r.get('validation') is not None]
    
    if valid_results:
        rel_L2_errors = [
            r['validation']['accuracy']['rel_L2_error']
            for r in valid_results
            if not (r['validation']['accuracy']['rel_L2_error'] != r['validation']['accuracy']['rel_L2_error'])  # not NaN
        ]
        
        avg_L2_error = sum(rel_L2_errors) / len(rel_L2_errors) if rel_L2_errors else float('nan')
        max_L2_error = max(rel_L2_errors) if rel_L2_errors else float('nan')
        min_L2_error = min(rel_L2_errors) if rel_L2_errors else float('nan')
    else:
        avg_L2_error = float('nan')
        max_L2_error = float('nan')
        min_L2_error = float('nan')
    
    # Group by level
    level_stats = {}
    for r in results:
        # Extract level from case_id (assumes format like "poisson_simple")
        # In a real implementation, this should come from the dataset entry
        level = "unknown"
        
        if level not in level_stats:
            level_stats[level] = {'total': 0, 'passed': 0}
        
        level_stats[level]['total'] += 1
        if r.get('success', False):
            level_stats[level]['passed'] += 1
    
    report = {
        'summary': {
            'total_cases': total_cases,
            'successful_cases': successful_cases,
            'failed_cases': failed_cases,
            'success_rate': success_rate,
        },
        'accuracy_statistics': {
            'avg_rel_L2_error': avg_L2_error,
            'min_rel_L2_error': min_L2_error,
            'max_rel_L2_error': max_L2_error,
        },
        'level_breakdown': level_stats,
        'cases': results,
    }
    
    with open(output_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    # Print summary to console
    print(f"\n{'='*60}")
    print("EVALUATION SUMMARY")
    print(f"{'='*60}")
    print(f"Total cases:      {total_cases}")
    print(f"Successful:       {successful_cases} ({success_rate*100:.1f}%)")
    print(f"Failed:           {failed_cases}")
    print(f"\nAccuracy Statistics:")
    print(f"  Avg rel L2 error: {avg_L2_error:.3e}")
    print(f"  Min rel L2 error: {min_L2_error:.3e}")
    print(f"  Max rel L2 error: {max_L2_error:.3e}")
    print(f"\nDetailed report saved to: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='Evaluate agent on PDE benchmark',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Evaluate with mock agent (for testing)
  python scripts/evaluate_agent.py --dataset datasets/level_2_1_basic.jsonl \\
                                    --mock-agent --outdir results/mock_test
  
  # Evaluate with real agent script
  python scripts/evaluate_agent.py --dataset datasets/level_2_1_basic.jsonl \\
                                    --agent-script my_agent_solver.py \\
                                    --outdir results/agent_run_001
        """
    )
    
    parser.add_argument(
        '--dataset',
        type=Path,
        required=True,
        help='Path to dataset JSONL file'
    )
    
    parser.add_argument(
        '--agent-script',
        type=Path,
        help='Path to agent script (if not using mock agent)'
    )
    
    parser.add_argument(
        '--mock-agent',
        action='store_true',
        help='Use mock agent (Oracle solver) for testing'
    )
    
    parser.add_argument(
        '--outdir',
        type=Path,
        required=True,
        help='Output directory for results'
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        help='Limit number of cases to evaluate (for testing)'
    )
    
    args = parser.parse_args()
    
    # Validate arguments
    if not args.mock_agent and not args.agent_script:
        parser.error("Must provide either --agent-script or --mock-agent")
    
    if not args.dataset.exists():
        parser.error(f"Dataset file not found: {args.dataset}")
    
    # Load dataset
    print(f"Loading dataset: {args.dataset}")
    entries = load_dataset(str(args.dataset))
    
    if args.limit:
        entries = entries[:args.limit]
    
    print(f"Loaded {len(entries)} cases")
    
    # Create output directory
    args.outdir.mkdir(parents=True, exist_ok=True)
    
    # Evaluate each case
    results = []
    
    for i, entry in enumerate(entries, 1):
        print(f"\n[{i}/{len(entries)}] Case: {entry.id}")
        
        case_outdir = args.outdir / entry.id
        
        # Generate mock agent script if needed
        if args.mock_agent:
            agent_script = case_outdir / 'mock_agent.py'
            agent_script.parent.mkdir(parents=True, exist_ok=True)
            generate_mock_agent_script(entry.id, entry.oracle_config, agent_script)
        else:
            agent_script = args.agent_script
        
        # Evaluate
        result = evaluate_single_case(
            case_id=entry.id,
            agent_script=agent_script,
            oracle_config=entry.oracle_config,
            evaluation_config=entry.evaluation_config,
            output_dir=case_outdir
        )
        
        results.append(result)
    
    # Generate summary report
    summary_file = args.outdir / 'summary.json'
    generate_summary_report(results, summary_file)


if __name__ == '__main__':
    main()

