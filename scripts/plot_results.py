#!/usr/bin/env python3
"""
PDEBench å®éªŒç»“æœå¯è§†åŒ–è„šæœ¬

è¯»å– experiment_history.jsonl å¹¶ç”Ÿæˆç”¨äº NeurIPS è®ºæ–‡çš„é«˜è´¨é‡å›¾è¡¨ï¼š
1. Time-Accuracy Trade-off (å¸•ç´¯æ‰˜å‰æ²¿)
2. Optimization Trajectory (ä¼˜åŒ–è½¨è¿¹)

ç”¨æ³•ï¼š
    python scripts/plot_results.py
    python scripts/plot_results.py --output figures/ --format pdf
"""
import json
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from datetime import datetime

# è®¾ç½®è®ºæ–‡çº§åˆ«çš„ç»˜å›¾é£æ ¼
mpl.rcParams['font.size'] = 11
mpl.rcParams['font.family'] = 'serif'
mpl.rcParams['axes.labelsize'] = 12
mpl.rcParams['axes.titlesize'] = 13
mpl.rcParams['xtick.labelsize'] = 10
mpl.rcParams['ytick.labelsize'] = 10
mpl.rcParams['legend.fontsize'] = 10
mpl.rcParams['figure.titlesize'] = 14


def load_experiment_history(log_file):
    """åŠ è½½å®éªŒå†å²è®°å½•"""
    experiments = []
    with open(log_file, 'r') as f:
        for line in f:
            if line.strip():
                experiments.append(json.loads(line))
    return experiments


def plot_pareto_front(experiments, output_dir, fmt='png'):
    """
    ç»˜åˆ¶ Time-Accuracy Trade-off å¸•ç´¯æ‰˜å‰æ²¿å›¾
    """
    fig, ax = plt.subplots(figsize=(8, 6))
    
    times = []
    errors = []
    labels = []
    colors_list = []
    
    # ä½¿ç”¨é¢œè‰²æ˜ å°„è¡¨ç¤ºæ—¶é—´é¡ºåº
    cmap = plt.cm.viridis
    n_exp = len(experiments)
    
    for idx, exp in enumerate(experiments):
        summary = exp['summary']
        
        # åªç»˜åˆ¶é€šè¿‡çš„å®éªŒ
        if summary['pass_rate'] == 1.0:
            times.append(summary['total_wall_time'])
            errors.append(summary['avg_rel_error'])
            labels.append(exp['experiment_id'])
            colors_list.append(cmap(idx / max(n_exp - 1, 1)))
    
    if not times:
        print("âš ï¸  æ²¡æœ‰æˆåŠŸçš„å®éªŒè®°å½•ï¼Œæ— æ³•ç»˜åˆ¶å¸•ç´¯æ‰˜å›¾")
        return
    
    # ç»˜åˆ¶æ•£ç‚¹ï¼Œé¢œè‰²è¡¨ç¤ºæ—¶é—´é¡ºåº
    scatter = ax.scatter(times, errors, c=range(len(times)), cmap='viridis', 
                         s=100, alpha=0.7, edgecolors='black', linewidth=1.5,
                         zorder=3)
    
    # æ ‡æ³¨ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªç‚¹
    if len(times) > 0:
        ax.annotate('Baseline', (times[0], errors[0]), 
                   xytext=(10, 10), textcoords='offset points',
                   fontsize=9, color='red', fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
    if len(times) > 1:
        ax.annotate('Latest', (times[-1], errors[-1]), 
                   xytext=(10, -15), textcoords='offset points',
                   fontsize=9, color='blue', fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))
    
    # æ‰¾åˆ°å¸•ç´¯æ‰˜å‰æ²¿
    pareto_indices = []
    for i in range(len(times)):
        is_pareto = True
        for j in range(len(times)):
            if i != j:
                # å¦‚æœå­˜åœ¨å¦ä¸€ä¸ªç‚¹æ—¢æ›´å¿«åˆæ›´å‡†ï¼Œå½“å‰ç‚¹å°±ä¸åœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Š
                if times[j] <= times[i] and errors[j] <= errors[i]:
                    if times[j] < times[i] or errors[j] < errors[i]:
                        is_pareto = False
                        break
        if is_pareto:
            pareto_indices.append(i)
    
    # ç»˜åˆ¶å¸•ç´¯æ‰˜å‰æ²¿è¿çº¿
    if len(pareto_indices) > 1:
        pareto_times = [times[i] for i in sorted(pareto_indices, key=lambda x: times[x])]
        pareto_errors = [errors[i] for i in sorted(pareto_indices, key=lambda x: times[x])]
        ax.plot(pareto_times, pareto_errors, 'r--', alpha=0.5, linewidth=2, 
               label='Pareto Front', zorder=2)
    
    ax.set_xlabel('Total Wall Time (seconds)', fontweight='bold')
    ax.set_ylabel('Average Relative Error (L2)', fontweight='bold')
    ax.set_title('Time-Accuracy Trade-off (Pareto Front)', fontweight='bold', pad=15)
    
    # ä½¿ç”¨ log scale ä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤º
    ax.set_xscale('log')
    ax.set_yscale('log')
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
    
    # æ·»åŠ é¢œè‰²æ¡è¡¨ç¤ºå®éªŒé¡ºåº
    cbar = plt.colorbar(scatter, ax=ax, label='Experiment Index (Chronological)')
    
    # æ·»åŠ å›¾ä¾‹
    ax.legend(loc='best', framealpha=0.9)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    output_path = output_dir / f'pareto_front.{fmt}'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¸•ç´¯æ‰˜å‰æ²¿å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_optimization_trajectory(experiments, output_dir, fmt='png'):
    """
    ç»˜åˆ¶ä¼˜åŒ–è½¨è¿¹å›¾ï¼Œå±•ç¤ºæ€§èƒ½éšå®éªŒæ­¥æ•°çš„å˜åŒ–
    """
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 10), sharex=True)
    
    steps = list(range(1, len(experiments) + 1))
    times = [exp['summary']['total_wall_time'] for exp in experiments]
    errors = [exp['summary']['avg_rel_error'] for exp in experiments]
    pass_rates = [exp['summary']['pass_rate'] * 100 for exp in experiments]
    iters = [exp['summary']['avg_iters'] for exp in experiments]
    
    # å­å›¾ 1: æ€»è€—æ—¶
    ax1.plot(steps, times, 'o-', color='#1f77b4', linewidth=2, markersize=6, label='Wall Time')
    ax1.fill_between(steps, times, alpha=0.3, color='#1f77b4')
    ax1.set_ylabel('Total Wall Time (s)', fontweight='bold')
    ax1.set_title('Optimization Trajectory Over Experiments', fontweight='bold', pad=15)
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.legend(loc='best')
    
    # æ ‡æ³¨æœ€ä¼˜ç‚¹
    min_time_idx = np.argmin(times)
    ax1.scatter([steps[min_time_idx]], [times[min_time_idx]], 
               color='red', s=150, zorder=5, marker='*', label='Best')
    
    # å­å›¾ 2: å¹³å‡è¯¯å·®
    ax2.plot(steps, errors, 's-', color='#ff7f0e', linewidth=2, markersize=6, label='Avg Rel Error')
    ax2.fill_between(steps, errors, alpha=0.3, color='#ff7f0e')
    ax2.set_ylabel('Avg Relative Error', fontweight='bold')
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3, linestyle='--')
    ax2.legend(loc='best')
    
    # å­å›¾ 3: é€šè¿‡ç‡ & å¹³å‡è¿­ä»£æ¬¡æ•°
    ax3_twin = ax3.twinx()
    
    line1 = ax3.plot(steps, pass_rates, '^-', color='#2ca02c', linewidth=2, 
                     markersize=6, label='Pass Rate (%)')
    line2 = ax3_twin.plot(steps, iters, 'd-', color='#d62728', linewidth=2, 
                          markersize=6, label='Avg Iterations')
    
    ax3.set_xlabel('Experiment Step', fontweight='bold')
    ax3.set_ylabel('Pass Rate (%)', fontweight='bold', color='#2ca02c')
    ax3_twin.set_ylabel('Avg Iterations', fontweight='bold', color='#d62728')
    
    ax3.tick_params(axis='y', labelcolor='#2ca02c')
    ax3_twin.tick_params(axis='y', labelcolor='#d62728')
    
    ax3.set_ylim([0, 105])
    ax3.grid(True, alpha=0.3, linestyle='--')
    
    # åˆå¹¶å›¾ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax3.legend(lines, labels, loc='best')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    output_path = output_dir / f'optimization_trajectory.{fmt}'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ä¼˜åŒ–è½¨è¿¹å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_per_case_comparison(experiments, output_dir, fmt='png'):
    """
    ç»˜åˆ¶æ¯ä¸ª case åœ¨ä¸åŒå®éªŒä¸­çš„æ€§èƒ½å¯¹æ¯”ï¼ˆä»…å¯¹æ¯”é¦–æ¬¡å’Œæœ€æ–°ï¼‰
    """
    if len(experiments) < 2:
        print("âš ï¸  å®éªŒè®°å½•å°‘äº 2 ä¸ªï¼Œè·³è¿‡ per-case å¯¹æ¯”å›¾")
        return
    
    baseline = experiments[0]
    latest = experiments[-1]
    
    case_ids = list(baseline['per_case'].keys())
    
    baseline_times = [baseline['per_case'][c]['wall_time'] for c in case_ids]
    latest_times = [latest['per_case'][c]['wall_time'] for c in case_ids]
    
    speedup = [b / l if l > 0 else 0 for b, l in zip(baseline_times, latest_times)]
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = np.arange(len(case_ids))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, baseline_times, width, label='Baseline', color='#1f77b4', alpha=0.8)
    bars2 = ax.bar(x + width/2, latest_times, width, label='Latest', color='#ff7f0e', alpha=0.8)
    
    ax.set_xlabel('Test Case', fontweight='bold')
    ax.set_ylabel('Wall Time (seconds)', fontweight='bold')
    ax.set_title('Per-Case Performance: Baseline vs Latest', fontweight='bold', pad=15)
    ax.set_xticks(x)
    ax.set_xticklabels(case_ids, rotation=45, ha='right')
    ax.legend(loc='best')
    ax.grid(True, axis='y', alpha=0.3, linestyle='--')
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨åŠ é€Ÿæ¯”
    for i, (b, l, s) in enumerate(zip(baseline_times, latest_times, speedup)):
        if s > 1:
            ax.text(i, max(b, l) * 1.05, f'{s:.2f}x', ha='center', fontsize=8, 
                   color='green', fontweight='bold')
    
    plt.tight_layout()
    
    output_path = output_dir / f'per_case_comparison.{fmt}'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Per-case å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="PDEBench å®éªŒç»“æœå¯è§†åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--log-file",
        default="experiment_history.jsonl",
        help="å®éªŒå†å²æ—¥å¿—æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output", "-o",
        default="figures",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--format",
        choices=['png', 'pdf', 'svg'],
        default='png',
        help="å›¾ç‰‡æ ¼å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    repo_root = Path(__file__).parent.parent
    log_file = repo_root / args.log_file
    output_dir = repo_root / args.output
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not log_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°å®éªŒå†å²æ–‡ä»¶: {log_file}")
        print(f"   è¯·å…ˆè¿è¡Œ: python scripts/benchmark_score.py --log-history")
        return 1
    
    # åŠ è½½å®éªŒå†å²
    print(f"ğŸ“– è¯»å–å®éªŒå†å²: {log_file}")
    experiments = load_experiment_history(log_file)
    print(f"   å…±æ‰¾åˆ° {len(experiments)} æ¡å®éªŒè®°å½•\n")
    
    if len(experiments) == 0:
        print("âŒ å®éªŒå†å²ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾")
        return 1
    
    # ç”Ÿæˆå›¾è¡¨
    print("ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾è¡¨...\n")
    
    plot_pareto_front(experiments, output_dir, args.format)
    plot_optimization_trajectory(experiments, output_dir, args.format)
    plot_per_case_comparison(experiments, output_dir, args.format)
    
    print(f"\nâœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæ¯•ï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

