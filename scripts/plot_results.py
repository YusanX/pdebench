#!/usr/bin/env python3
"""
PDEBench å®éªŒç»“æœå¯è§†åŒ–è„šæœ¬

è¯»å– experiment_history.jsonl å¹¶ç”Ÿæˆç”¨äº NeurIPS è®ºæ–‡çš„é«˜è´¨é‡å›¾è¡¨ï¼š
1. Time-Accuracy Trade-off (å¸•ç´¯æ‰˜å‰æ²¿)
2. Optimization Trajectory (ä¼˜åŒ–è½¨è¿¹)

ç”¨æ³•ï¼š
    python scripts/plot_results.py
    python scripts/plot_results.py --output figures/ --format pdf
"""
import json
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from datetime import datetime

# è®¾ç½®è®ºæ–‡çº§åˆ«çš„ç»˜å›¾é£æ ¼
mpl.rcParams['font.size'] = 11
mpl.rcParams['font.family'] = 'serif'
mpl.rcParams['axes.labelsize'] = 12
mpl.rcParams['axes.titlesize'] = 13
mpl.rcParams['xtick.labelsize'] = 10
mpl.rcParams['ytick.labelsize'] = 10
mpl.rcParams['legend.fontsize'] = 10
mpl.rcParams['figure.titlesize'] = 14


def load_experiment_history(log_file):
    """åŠ è½½å®éªŒå†å²è®°å½•"""
    experiments = []
    with open(log_file, 'r') as f:
        for line in f:
            if line.strip():
                experiments.append(json.loads(line))
    return experiments


def plot_pareto_front(experiments, output_dir, fmt='png'):
    """
    ç»˜åˆ¶ Time-Accuracy Trade-off å¸•ç´¯æ‰˜å‰æ²¿å›¾
    ä¸åŒæ¨¡å‹ç”¨ä¸åŒé¢œè‰²æ ‡è®°
    """
    fig, ax = plt.subplots(figsize=(10, 7))
    
    # æŒ‰æ¨¡å‹å®¶æ—åˆ†ç»„å¹¶æ”¶é›†æ•°æ®
    model_data = {
        'Baseline': {'times': [], 'errors': [], 'labels': [], 'color': '#1f77b4', 'marker': 'o'},
        'GPT-5.2': {'times': [], 'errors': [], 'labels': [], 'color': '#2ca02c', 'marker': 's'},
        'Gemini-2.5-Pro': {'times': [], 'errors': [], 'labels': [], 'color': '#ff7f0e', 'marker': '^'}
    }
    
    for exp in experiments:
        summary = exp['summary']
        exp_id = exp['experiment_id']
        
        # ç¡®å®šæ¨¡å‹å®¶æ—
        if 'gpt' in exp_id.lower():
            family = 'GPT-5.2'
        elif 'gemini' in exp_id.lower() or 'superassistant' in exp_id.lower():
            family = 'Gemini-2.5-Pro'
        elif 'baseline' == exp_id:
            family = 'Baseline'
        else:
            continue
        
        model_data[family]['times'].append(summary['total_wall_time'])
        model_data[family]['errors'].append(summary['avg_rel_error'])
        model_data[family]['labels'].append(exp_id)
    
    # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹å®¶æ—
    all_times = []
    all_errors = []
    
    for family, data in model_data.items():
        if not data['times']:
            continue
        
        # ç»˜åˆ¶æ•£ç‚¹
        ax.scatter(data['times'], data['errors'], 
                  c=data['color'], marker=data['marker'], s=150, 
                  alpha=0.7, edgecolors='black', linewidth=1.5,
                  label=family, zorder=3)
        
        all_times.extend(data['times'])
        all_errors.extend(data['errors'])
        
        # æ ‡æ³¨è¯¥å®¶æ—çš„æœ€ä¼˜ç‚¹ï¼ˆæœ€å¿«çš„ï¼‰
        if len(data['times']) > 0:
            best_idx = np.argmin(data['times'])
            best_time = data['times'][best_idx]
            best_error = data['errors'][best_idx]
            
            # ç®€åŒ–æ ‡ç­¾åç§°
            label_text = family
            if family == 'Baseline':
                label_text = 'Baseline'
            elif family == 'GPT-5.2':
                label_text = 'GPT-5.2\n(100% pass)'
            elif family == 'Gemini-2.5-Pro':
                label_text = f'Gemini-2.5-Pro\n({best_time:.2f}s, 90% pass)'
            
            # è°ƒæ•´æ ‡æ³¨ä½ç½®é¿å…é‡å 
            xytext_offset = (15, 10) if family != 'Gemini-2.5-Pro' else (15, -20)
            
            ax.annotate(label_text, (best_time, best_error), 
                       xytext=xytext_offset, textcoords='offset points',
                       fontsize=9, fontweight='bold',
                       bbox=dict(boxstyle='round,pad=0.4', 
                                facecolor=data['color'], alpha=0.3, 
                                edgecolor=data['color'], linewidth=2),
                       arrowprops=dict(arrowstyle='->', 
                                      connectionstyle='arc3,rad=0.3',
                                      color=data['color'], lw=1.5))
    
    if not all_times:
        print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒè®°å½•ï¼Œæ— æ³•ç»˜åˆ¶å¸•ç´¯æ‰˜å›¾")
        return
    
    # æ‰¾åˆ°å¸•ç´¯æ‰˜å‰æ²¿
    pareto_indices = []
    for i in range(len(all_times)):
        is_pareto = True
        for j in range(len(all_times)):
            if i != j:
                # å¦‚æœå­˜åœ¨å¦ä¸€ä¸ªç‚¹æ—¢æ›´å¿«åˆæ›´å‡†ï¼Œå½“å‰ç‚¹å°±ä¸åœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Š
                if all_times[j] <= all_times[i] and all_errors[j] <= all_errors[i]:
                    if all_times[j] < all_times[i] or all_errors[j] < all_errors[i]:
                        is_pareto = False
                        break
        if is_pareto:
            pareto_indices.append(i)
    
    # ç»˜åˆ¶å¸•ç´¯æ‰˜å‰æ²¿è¿çº¿
    if len(pareto_indices) > 1:
        pareto_times = [all_times[i] for i in sorted(pareto_indices, key=lambda x: all_times[x])]
        pareto_errors = [all_errors[i] for i in sorted(pareto_indices, key=lambda x: all_times[x])]
        ax.plot(pareto_times, pareto_errors, 'r--', alpha=0.5, linewidth=2, 
               label='Pareto Front', zorder=2)
    
    ax.set_xlabel('Total Wall Time (seconds)', fontweight='bold', fontsize=12)
    ax.set_ylabel('Average Relative Error (L2)', fontweight='bold', fontsize=12)
    ax.set_title('Time-Accuracy Trade-off: LLM Agent Comparison', fontweight='bold', pad=15, fontsize=14)
    
    # ä½¿ç”¨ log scale ä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤º
    ax.set_xscale('log')
    ax.set_yscale('log')
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
    
    # æ·»åŠ å›¾ä¾‹
    ax.legend(loc='best', framealpha=0.9, fontsize=11)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    output_path = output_dir / f'pareto_front.{fmt}'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¸•ç´¯æ‰˜å‰æ²¿å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_optimization_trajectory(experiments, output_dir, fmt='png'):
    """
    ç»˜åˆ¶ä¼˜åŒ–è½¨è¿¹å›¾ï¼Œå±•ç¤ºæ€§èƒ½éšå®éªŒæ­¥æ•°çš„å˜åŒ–
    """
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 10), sharex=True)
    
    steps = list(range(1, len(experiments) + 1))
    times = [exp['summary']['total_wall_time'] for exp in experiments]
    errors = [exp['summary']['avg_rel_error'] for exp in experiments]
    pass_rates = [exp['summary']['pass_rate'] * 100 for exp in experiments]
    iters = [exp['summary']['avg_iters'] for exp in experiments]
    
    # å­å›¾ 1: æ€»è€—æ—¶
    ax1.plot(steps, times, 'o-', color='#1f77b4', linewidth=2, markersize=6, label='Wall Time')
    ax1.fill_between(steps, times, alpha=0.3, color='#1f77b4')
    ax1.set_ylabel('Total Wall Time (s)', fontweight='bold')
    ax1.set_title('Optimization Trajectory Over Experiments', fontweight='bold', pad=15)
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.legend(loc='best')
    
    # æ ‡æ³¨æœ€ä¼˜ç‚¹
    min_time_idx = np.argmin(times)
    ax1.scatter([steps[min_time_idx]], [times[min_time_idx]], 
               color='red', s=150, zorder=5, marker='*', label='Best')
    
    # å­å›¾ 2: å¹³å‡è¯¯å·®
    ax2.plot(steps, errors, 's-', color='#ff7f0e', linewidth=2, markersize=6, label='Avg Rel Error')
    ax2.fill_between(steps, errors, alpha=0.3, color='#ff7f0e')
    ax2.set_ylabel('Avg Relative Error', fontweight='bold')
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3, linestyle='--')
    ax2.legend(loc='best')
    
    # å­å›¾ 3: é€šè¿‡ç‡ & å¹³å‡è¿­ä»£æ¬¡æ•°
    ax3_twin = ax3.twinx()
    
    line1 = ax3.plot(steps, pass_rates, '^-', color='#2ca02c', linewidth=2, 
                     markersize=6, label='Pass Rate (%)')
    line2 = ax3_twin.plot(steps, iters, 'd-', color='#d62728', linewidth=2, 
                          markersize=6, label='Avg Iterations')
    
    ax3.set_xlabel('Experiment Step', fontweight='bold')
    ax3.set_ylabel('Pass Rate (%)', fontweight='bold', color='#2ca02c')
    ax3_twin.set_ylabel('Avg Iterations', fontweight='bold', color='#d62728')
    
    ax3.tick_params(axis='y', labelcolor='#2ca02c')
    ax3_twin.tick_params(axis='y', labelcolor='#d62728')
    
    ax3.set_ylim([0, 105])
    ax3.grid(True, alpha=0.3, linestyle='--')
    
    # åˆå¹¶å›¾ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax3.legend(lines, labels, loc='best')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    output_path = output_dir / f'optimization_trajectory.{fmt}'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ä¼˜åŒ–è½¨è¿¹å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_per_case_comparison(experiments, output_dir, fmt='png'):
    """
    ç»˜åˆ¶æ¯ä¸ª case åœ¨ä¸åŒå®éªŒä¸­çš„æ€§èƒ½å¯¹æ¯”
    æ™ºèƒ½é€‰æ‹©: baseline + æ¯ä¸ªæ¨¡å‹å®¶æ—çš„æœ€ä¼˜ç»“æœ
    """
    if len(experiments) < 2:
        print("âš ï¸  å®éªŒè®°å½•å°‘äº 2 ä¸ªï¼Œè·³è¿‡ per-case å¯¹æ¯”å›¾")
        return
    
    # æŒ‰æ¨¡å‹å®¶æ—åˆ†ç»„
    model_families = {}
    for exp in experiments:
        exp_id = exp['experiment_id']
        # æå–æ¨¡å‹åç§°ï¼ˆå»æ‰åç¼€ï¼‰
        if 'gpt' in exp_id.lower():
            family = 'GPT-5.2'
        elif 'gemini' in exp_id.lower() or 'superassistant' in exp_id.lower():
            family = 'Gemini-2.5-Pro'
        elif 'baseline' == exp_id:
            family = 'Baseline'
        else:
            family = 'Other'
        
        if family not in model_families:
            model_families[family] = []
        model_families[family].append(exp)
    
    # é€‰æ‹©æ¯ä¸ªå®¶æ—çš„æœ€ä¼˜ç»“æœï¼ˆæœ€çŸ­æ—¶é—´ + 100% pass rateï¼‰
    selected_experiments = []
    selected_labels = []
    
    # å…ˆæ·»åŠ  Baseline
    if 'Baseline' in model_families:
        selected_experiments.append(model_families['Baseline'][0])
        selected_labels.append('Baseline')
    
    # å†æ·»åŠ å…¶ä»–æ¨¡å‹çš„æœ€ä¼˜ç»“æœ
    for family in ['GPT-5.2', 'Gemini-2.5-Pro', 'Other']:
        if family in model_families:
            # ä¼˜å…ˆé€‰æ‹© 100% pass rate çš„ï¼Œç„¶åé€‰æ‹©æœ€çŸ­æ—¶é—´çš„
            family_exps = model_families[family]
            best = min(family_exps, key=lambda e: (
                -e['summary']['pass_rate'],  # ä¼˜å…ˆé«˜é€šè¿‡ç‡
                e['summary']['total_wall_time']  # å…¶æ¬¡ä½æ—¶é—´
            ))
            selected_experiments.append(best)
            selected_labels.append(f"{family} (best)")
    
    if len(selected_experiments) < 2:
        print("âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„å®éªŒè¿›è¡Œå¯¹æ¯”")
        return
    
    # è·å–æ‰€æœ‰ case
    case_ids = list(selected_experiments[0]['per_case'].keys())
    
    # å‡†å¤‡ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
    
    x = np.arange(len(case_ids))
    width = 0.8 / len(selected_experiments)
    
    colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728', '#9467bd']
    
    # å­å›¾ 1: è€—æ—¶å¯¹æ¯”
    for idx, (exp, label) in enumerate(zip(selected_experiments, selected_labels)):
        times = [exp['per_case'][c]['wall_time'] for c in case_ids]
        offset = (idx - len(selected_experiments)/2 + 0.5) * width
        ax1.bar(x + offset, times, width, label=label, 
               color=colors[idx % len(colors)], alpha=0.8)
    
    ax1.set_ylabel('Wall Time (seconds)', fontweight='bold')
    ax1.set_title('Per-Case Performance Comparison: Wall Time', fontweight='bold', pad=15)
    ax1.set_xticks(x)
    ax1.set_xticklabels(case_ids, rotation=45, ha='right')
    ax1.legend(loc='best', framealpha=0.9)
    ax1.grid(True, axis='y', alpha=0.3, linestyle='--')
    
    # å­å›¾ 2: è¿­ä»£æ¬¡æ•°å¯¹æ¯”
    for idx, (exp, label) in enumerate(zip(selected_experiments, selected_labels)):
        iters = [exp['per_case'][c]['iters'] for c in case_ids]
        offset = (idx - len(selected_experiments)/2 + 0.5) * width
        ax2.bar(x + offset, iters, width, label=label, 
               color=colors[idx % len(colors)], alpha=0.8)
    
    ax2.set_xlabel('Test Case', fontweight='bold')
    ax2.set_ylabel('Iterations', fontweight='bold')
    ax2.set_title('Per-Case Performance Comparison: Iterations', fontweight='bold', pad=15)
    ax2.set_xticks(x)
    ax2.set_xticklabels(case_ids, rotation=45, ha='right')
    ax2.legend(loc='best', framealpha=0.9)
    ax2.grid(True, axis='y', alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    
    output_path = output_dir / f'per_case_comparison.{fmt}'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Per-case å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    print(f"   å¯¹æ¯”æ¨¡å‹: {', '.join(selected_labels)}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="PDEBench å®éªŒç»“æœå¯è§†åŒ–",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--log-file",
        default="experiment_history.jsonl",
        help="å®éªŒå†å²æ—¥å¿—æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output", "-o",
        default="figures",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--format",
        choices=['png', 'pdf', 'svg'],
        default='png',
        help="å›¾ç‰‡æ ¼å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    repo_root = Path(__file__).parent.parent
    log_file = repo_root / args.log_file
    output_dir = repo_root / args.output
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not log_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°å®éªŒå†å²æ–‡ä»¶: {log_file}")
        print(f"   è¯·å…ˆè¿è¡Œ: python scripts/benchmark_score.py --log-history")
        return 1
    
    # åŠ è½½å®éªŒå†å²
    print(f"ğŸ“– è¯»å–å®éªŒå†å²: {log_file}")
    experiments = load_experiment_history(log_file)
    print(f"   å…±æ‰¾åˆ° {len(experiments)} æ¡å®éªŒè®°å½•\n")
    
    if len(experiments) == 0:
        print("âŒ å®éªŒå†å²ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾")
        return 1
    
    # ç”Ÿæˆå›¾è¡¨
    print("ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾è¡¨...\n")
    
    plot_pareto_front(experiments, output_dir, args.format)
    plot_optimization_trajectory(experiments, output_dir, args.format)
    plot_per_case_comparison(experiments, output_dir, args.format)
    
    print(f"\nâœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæ¯•ï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

