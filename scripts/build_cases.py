#!/usr/bin/env python3
"""
Cases æž„å»ºè„šæœ¬

ä»Ž pdebench/data/benchmark.jsonl ç”Ÿæˆå®Œæ•´çš„ cases/ ç›®å½•

æ ¸å¿ƒé€»è¾‘ï¼š
1. è¯»å–æºæ•°æ® (JSONL)
2. è¿è¡Œ Oracle èŽ·å–åŸºå‡†æ€§èƒ½
3. è®¡ç®—åŠ¨æ€éš¾åº¦åˆ†çº§
4. ç”Ÿæˆ config.json, description.md, test_*.py
"""

import argparse
import json
import sys
import tempfile
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ  pdebench åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from pdebench.templates.prompts import generate_description_md
from pdebench.templates.scripts import generate_test_script


def run_oracle(oracle_config: Dict[str, Any], case_id: str, entry: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¿è¡Œ Oracle æ±‚è§£å™¨èŽ·å–åŸºå‡†æ€§èƒ½
    
    Returns:
        {
            'error': float,  # å‚è€ƒè¯¯å·®
            'time': float    # å‚è€ƒæ—¶é—´
        }
    """
    print(f"   ðŸ”® Running Oracle for {case_id}...")
    
    try:
        from pdebench.oracle.core.generate import generate
        from pdebench.oracle.core.solve import solve_case
        from pdebench.oracle.core.evaluate import evaluate
        
        # åœ¨ä¸´æ—¶ç›®å½•è¿è¡Œ Oracle
        with tempfile.TemporaryDirectory() as tmpdir:
            tmppath = Path(tmpdir)
            
            # å‡†å¤‡å®Œæ•´çš„ Oracle é…ç½®ï¼ˆéœ€è¦è¡¥å…… targets å­—æ®µï¼‰
            # å› ä¸º evaluate() æœŸæœ› case_spec['targets']
            oracle_config_full = dict(oracle_config)
            if 'targets' not in oracle_config_full:
                # ä»Ž evaluation_config æŽ¨æ–­ targets
                eval_cfg = entry.get('evaluation_config', {})
                oracle_config_full['targets'] = {
                    'metric': eval_cfg.get('target_metric', 'rel_L2_fe'),
                    'target_error': 1e-2  # ä¸´æ—¶å€¼ï¼Œåæ­£æˆ‘ä»¬åªè¦è¯¯å·®æ•°æ®
                }
            
            # è¿è¡Œä¸‰é˜¶æ®µ Oracle pipeline
            generate(oracle_config_full, tmppath)
            solve_case(oracle_config_full, tmppath)
            metrics = evaluate(oracle_config_full, tmppath)
            
            # è¯»å–å…ƒæ•°æ®èŽ·å–æ—¶é—´
            with open(tmppath / 'meta.json') as f:
                meta = json.load(f)
            
            # æå–åŸºå‡†æŒ‡æ ‡
            target_metric = oracle_config_full['targets']['metric']
            error_ref = metrics.get(target_metric, metrics.get('rel_L2_fe', 1e-3))
            time_ref = meta.get('wall_time_sec', 10.0)
            
            print(f"   âœ… Oracle baseline: E_ref={error_ref:.2e}, T_ref={time_ref:.3f}s")
            
            return {
                'error': float(error_ref),
                'time': float(time_ref)
            }
            
    except Exception as e:
        print(f"   âš ï¸  Oracle failed, using default baseline: {e}")
        import traceback
        traceback.print_exc()
        # å¦‚æžœ Oracle å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆçš„é»˜è®¤å€¼
        return {
            'error': 1e-2,
            'time': 10.0
        }


def calculate_difficulty_tiers(baseline_error: float, baseline_time: float) -> Dict[str, Any]:
    """
    åŸºäºŽ Oracle åŸºå‡†åŠ¨æ€è®¡ç®—éš¾åº¦åˆ†çº§
    
    Args:
        baseline_error: Oracle çš„å‚è€ƒè¯¯å·®
        baseline_time: Oracle çš„å‚è€ƒæ—¶é—´
        
    Returns:
        {
            'accuracy': {
                'level_1': {'target_error': ..., 'name': 'Low/Engineering'},
                'level_2': {'target_error': ..., 'name': 'Medium/Standard'},
                'level_3': {'target_error': ..., 'name': 'High/Scientific'}
            },
            'speed': {
                'fast': {'time_budget': ..., 'name': 'Real-time'},
                'medium': {'time_budget': ..., 'name': 'Interactive'},
                'slow': {'time_budget': ..., 'name': 'Batch'}
            }
        }
    """
    return {
        'accuracy': {
            'level_1': {
                'target_error': baseline_error * 100,
                'name': 'Low/Engineering'
            },
            'level_2': {
                'target_error': baseline_error * 1.0,
                'name': 'Medium/Standard'
            },
            'level_3': {
                'target_error': baseline_error * 0.01,
                'name': 'High/Scientific'
            }
        },
        'speed': {
            'fast': {
                'time_budget': baseline_time * 0.1,
                'name': 'Real-time'
            },
            'medium': {
                'time_budget': baseline_time * 1.0,
                'name': 'Interactive'
            },
            'slow': {
                'time_budget': baseline_time * 10.0,
                'name': 'Batch'
            }
        }
    }


def build_case(entry: Dict[str, Any], output_dir: Path, skip_oracle: bool = False):
    """
    æž„å»ºå•ä¸ª case
    
    Args:
        entry: ä»Ž benchmark.jsonl è¯»å–çš„æ¡ç›®
        output_dir: è¾“å‡ºç›®å½• (cases/)
        skip_oracle: æ˜¯å¦è·³è¿‡ Oracle è¿è¡Œï¼ˆç”¨äºŽå¿«é€Ÿæµ‹è¯•ï¼‰
    """
    case_id = entry['id']
    case_dir = output_dir / case_id
    case_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nðŸ“ Building case: {case_id}")
    
    # Step 1: è¿è¡Œ Oracle (æˆ–ä½¿ç”¨é»˜è®¤å€¼)
    if not skip_oracle:
        baseline = run_oracle(entry['oracle_config'], case_id, entry)
    else:
        print(f"   âš¡ Skipping Oracle (using default baseline)")
        baseline = {'error': 1e-2, 'time': 10.0}
    
    # Step 2: è®¡ç®—éš¾åº¦åˆ†çº§
    difficulty_tiers = calculate_difficulty_tiers(baseline['error'], baseline['time'])
    
    # Step 3: æž„å»ºå®Œæ•´ config.json
    full_config = {
        **entry,  # åŒ…å«æ‰€æœ‰æºæ•°æ®
        'baseline': {
            'error_ref': baseline['error'],
            'time_ref': baseline['time'],
            'description': 'Oracle baseline performance'
        },
        'difficulty_tiers': difficulty_tiers,
        'evaluation_config': {
            **entry['evaluation_config'],
            'target_error': difficulty_tiers['accuracy']['level_2']['target_error']  # é»˜è®¤ç›®æ ‡ä¸º level_2
        }
    }
    
    with open(case_dir / 'config.json', 'w') as f:
        json.dump(full_config, f, indent=2)
    print(f"   âœ… config.json")
    
    # Step 4: ç”Ÿæˆ description.md
    description = generate_description_md(
        entry,
        target_error=difficulty_tiers['accuracy']['level_2']['target_error'],
        difficulty_tiers=difficulty_tiers
    )
    with open(case_dir / 'description.md', 'w') as f:
        f.write(description)
    print(f"   âœ… description.md")
    
    # Step 5: ç”Ÿæˆæµ‹è¯•è„šæœ¬
    for mode in ['fix_accuracy', 'fix_time']:
        script = generate_test_script(entry, mode)
        script_path = case_dir / f'test_{mode}.py'
        with open(script_path, 'w') as f:
            f.write(script)
        script_path.chmod(0o755)
        print(f"   âœ… test_{mode}.py")
    
    print(f"   âœ¨ Case built successfully")


def main():
    parser = argparse.ArgumentParser(
        description='Build cases from benchmark.jsonl',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--data',
        type=Path,
        default=Path('data/benchmark.jsonl'),
        help='Path to benchmark.jsonl (default: data/benchmark.jsonl)'
    )
    
    parser.add_argument(
        '--output',
        type=Path,
        default=Path('cases'),
        help='Output directory (default: cases/)'
    )
    
    parser.add_argument(
        '--cases',
        nargs='+',
        help='Build specific cases only (default: all)'
    )
    
    parser.add_argument(
        '--skip-oracle',
        action='store_true',
        help='Skip Oracle execution, use default baselines (faster for testing)'
    )
    
    args = parser.parse_args()
    
    # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•
    root_dir = Path(__file__).parent.parent
    data_file = root_dir / args.data
    output_dir = root_dir / args.output
    
    if not data_file.exists():
        print(f"âŒ Error: Data file not found: {data_file}")
        print(f"   Please run: python scripts/migrate_to_data.py first")
        sys.exit(1)
    
    print(f"\n{'='*80}")
    print("ðŸ—ï¸  PDEBench Cases Builder")
    print(f"{'='*80}")
    print(f"ðŸ“„ Data: {data_file}")
    print(f"ðŸ“ Output: {output_dir}")
    if args.skip_oracle:
        print(f"âš¡ Mode: Fast (skipping Oracle)")
    print(f"{'='*80}")
    
    # è¯»å–æ•°æ®
    entries = []
    with open(data_file) as f:
        for line in f:
            if line.strip():
                entries.append(json.loads(line))
    
    print(f"\nðŸ“Š Found {len(entries)} cases in dataset")
    
    # è¿‡æ»¤ cases
    if args.cases:
        entries = [e for e in entries if e['id'] in args.cases]
        if not entries:
            print(f"âŒ Error: None of the specified cases found")
            sys.exit(1)
        print(f"   Building {len(entries)} selected cases")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æž„å»ºæ‰€æœ‰ cases
    success_count = 0
    for entry in entries:
        try:
            build_case(entry, output_dir, args.skip_oracle)
            success_count += 1
        except Exception as e:
            print(f"   âŒ Failed: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*80}")
    print(f"âœ… Successfully built {success_count}/{len(entries)} cases")
    print(f"{'='*80}\n")
    
    if success_count > 0:
        print("ðŸ“– Usage example:")
        example_case = entries[0]['id']
        print(f"   cd {output_dir}/{example_case}")
        print(f"   python test_fix_accuracy.py --agent-script /path/to/solver.py")


if __name__ == '__main__':
    main()

