#!/usr/bin/env python3
"""
PDEBench æ€§èƒ½è¯„ä¼°è„šæœ¬

ç”¨äºé‡åŒ–æ±‚è§£å™¨æ€§èƒ½ï¼Œä¸»è¦æŒ‡æ ‡ï¼š
1. æ€»è€—æ—¶ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
2. é€šè¿‡ç‡ï¼ˆå¿…é¡» 100%ï¼‰
3. å¹³å‡è¿­ä»£æ¬¡æ•°

ç”¨æ³•ï¼š
    python scripts/benchmark_score.py [--output report.json] [--keep-artifacts]
    python scripts/benchmark_score.py --log-history --experiment-id "gpt4_run1"
"""
import sys
import json
import subprocess
import time
import argparse
from pathlib import Path
import shutil
import datetime
import numpy as np


def run_benchmark(keep_artifacts=False, output_file=None, log_history=False, experiment_id=None):
    """è¿è¡Œå®Œæ•´çš„ benchmark å¥—ä»¶"""
    repo_root = Path(__file__).parent.parent
    demo_dir = repo_root / "cases" / "demo"
    cases = list(demo_dir.glob("*.json"))
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œä¿è¯é¡ºåºä¸€è‡´
    cases.sort()
    
    print("=" * 80)
    print("ğŸš€ PDEBench æ€§èƒ½è¯„ä¼°")
    print("=" * 80)
    print(f"æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(cases)}")
    print(f"é¡¹ç›®è·¯å¾„: {repo_root}")
    print("=" * 80)
    print()
    
    # è¡¨å¤´
    print(f"{'Case ID':<25} | {'çŠ¶æ€':<8} | {'è€—æ—¶(s)':<10} | {'è¿­ä»£':<8} | {'å¤‡æ³¨'}")
    print("-" * 80)
    
    results = []
    total_wall_time = 0.0
    total_iters = 0
    passed_cases = 0
    failed_cases = []
    
    # ç»“æœè¾“å‡ºç›®å½•
    artifacts_dir = repo_root / "artifacts_bench"
    if artifacts_dir.exists() and not keep_artifacts:
        shutil.rmtree(artifacts_dir)
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    
    for case in cases:
        case_id = case.stem
        outdir = artifacts_dir / case_id
        
        # è¿è¡Œ CLI run å‘½ä»¤
        cmd = [
            sys.executable, "-m", "pdebench.cli", "run",
            str(case),
            "--outdir", str(outdir)
        ]
        
        start_time = time.time()
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            cwd=repo_root,
            timeout=60  # å•ä¸ª case æœ€å¤š 60 ç§’
        )
        elapsed_time = time.time() - start_time
        
        # åˆå§‹åŒ–ç»“æœè®°å½•
        case_result = {
            "case_id": case_id,
            "status": "UNKNOWN",
            "wall_time": 10.0,  # é»˜è®¤æƒ©ç½šæ—¶é—´
            "iters": -1,
            "note": "",
            "metrics": {}
        }
        
        if result.returncode == 0:
            try:
                # è¯»å– metrics.json
                metrics_file = outdir / "metrics.json"
                if metrics_file.exists():
                    with open(metrics_file) as f:
                        metrics = json.load(f)
                    
                    case_result["metrics"] = metrics
                    
                    if metrics["validity"]["pass"]:
                        case_result["status"] = "PASS"
                        case_result["wall_time"] = metrics["cost"]["wall_time_sec"]
                        case_result["iters"] = metrics["cost"]["iters"]
                        case_result["note"] = f"res={metrics.get('rel_res', -1):.2e}"
                        
                        passed_cases += 1
                        total_wall_time += case_result["wall_time"]
                        total_iters += case_result["iters"]
                    else:
                        case_result["status"] = "FAIL"
                        case_result["note"] = metrics["validity"]["reason"][:40]
                        failed_cases.append(case_id)
                else:
                    case_result["status"] = "NO_METRICS"
                    case_result["note"] = "metrics.json not found"
                    failed_cases.append(case_id)
            except Exception as e:
                case_result["status"] = "ERROR"
                case_result["note"] = str(e)[:40]
                failed_cases.append(case_id)
        else:
            # è¿è¡Œå´©æºƒ
            case_result["status"] = "CRASH"
            # æå–æœ€åä¸€è¡Œé”™è¯¯ä¿¡æ¯
            stderr_lines = result.stderr.strip().split('\n')
            if stderr_lines:
                # æ‰¾åˆ°æœ€åä¸€ä¸ªéç©ºè¡Œ
                for line in reversed(stderr_lines):
                    if line.strip():
                        case_result["note"] = line.strip()[-40:]
                        break
            else:
                case_result["note"] = "Unknown error"
            failed_cases.append(case_id)
        
        results.append(case_result)
        
        # æ‰“å°ç»“æœè¡Œ
        status_emoji = "âœ…" if case_result["status"] == "PASS" else "âŒ"
        print(f"{status_emoji} {case_result['case_id']:<23} | "
              f"{case_result['status']:<8} | "
              f"{case_result['wall_time']:<10.4f} | "
              f"{case_result['iters']:<8} | "
              f"{case_result['note']}")
    
    print("-" * 80)
    print()
    
    # æ±‡æ€»ç»Ÿè®¡
    success_rate = passed_cases / len(cases) * 100
    avg_iters = total_iters / passed_cases if passed_cases > 0 else 0
    
    print("=" * 80)
    print("ğŸ† æœ€ç»ˆå¾—åˆ†æ‘˜è¦")
    print("=" * 80)
    print(f"ğŸ“Š æ€»è€—æ—¶ (è¶Šä½è¶Šå¥½):  {total_wall_time:.4f} ç§’")
    print(f"âœ“  é€šè¿‡ç‡:             {passed_cases}/{len(cases)} ({success_rate:.1f}%)")
    print(f"ğŸ”„ å¹³å‡è¿­ä»£æ¬¡æ•°:       {avg_iters:.1f}")
    
    if failed_cases:
        print(f"âŒ å¤±è´¥çš„ Cases:       {', '.join(failed_cases)}")
    
    print("=" * 80)
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ° JSON
    report = {
        "summary": {
            "total_cases": len(cases),
            "passed_cases": passed_cases,
            "failed_cases": len(failed_cases),
            "success_rate": success_rate,
            "total_wall_time": total_wall_time,
            "avg_iters": avg_iters,
        },
        "failed_list": failed_cases,
        "details": results
    }
    
    if output_file:
        output_path = Path(output_file)
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    # è®°å½•åˆ°å†å²æ—¥å¿—ï¼ˆç”¨äºç»˜å›¾å’Œè¿½è¸ªï¼‰
    if log_history:
        log_experiment_step(
            results=results,
            total_wall_time=total_wall_time,
            passed_cases=passed_cases,
            total_cases=len(cases),
            avg_iters=avg_iters,
            experiment_id=experiment_id,
            repo_root=repo_root
        )
    
    # è¿”å›çŠ¶æ€ç 
    if failed_cases:
        print("\nâš ï¸  æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¿”å›çŠ¶æ€ç  1")
        return 1
    else:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0


def log_experiment_step(results, total_wall_time, passed_cases, total_cases, avg_iters, experiment_id, repo_root):
    """å°†å®éªŒç»“æœè¿½åŠ åˆ°å†å²æ—¥å¿—æ–‡ä»¶"""
    
    # è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®ï¼ˆå‡ ä½•å¹³å‡ï¼Œå¯¹ log-scale æ›´åˆç†ï¼‰
    rel_errors = []
    for r in results:
        if r["status"] == "PASS" and "rel_L2_fe" in r["metrics"]:
            rel_errors.append(r["metrics"]["rel_L2_fe"])
    
    if rel_errors:
        # å‡ ä½•å¹³å‡ï¼šexp(mean(log(errors)))
        avg_rel_error = np.exp(np.mean(np.log(rel_errors)))
    else:
        avg_rel_error = float('inf')
    
    # ç”Ÿæˆå®éªŒ ID
    if experiment_id is None:
        experiment_id = f"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # æ„å»ºæ—¥å¿—æ¡ç›®
    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "experiment_id": experiment_id,
        "summary": {
            "total_wall_time": total_wall_time,
            "avg_rel_error": avg_rel_error,
            "pass_rate": passed_cases / total_cases,
            "passed_cases": passed_cases,
            "total_cases": total_cases,
            "avg_iters": avg_iters
        },
        "per_case": {
            r["case_id"]: {
                "status": r["status"],
                "wall_time": r["wall_time"],
                "iters": r["iters"],
                "rel_L2_fe": r["metrics"].get("rel_L2_fe", None) if r["status"] == "PASS" else None,
                "rel_res": r["metrics"].get("rel_res", None) if r["status"] == "PASS" else None
            }
            for r in results
        }
    }
    
    # è¿½åŠ åˆ° JSONL æ–‡ä»¶
    log_file = repo_root / "experiment_history.jsonl"
    with open(log_file, "a") as f:
        f.write(json.dumps(log_entry) + "\n")
    
    print(f"\nğŸ“ˆ å®éªŒè®°å½•å·²è¿½åŠ åˆ°: {log_file}")
    print(f"   å®éªŒ ID: {experiment_id}")
    print(f"   å¹³å‡ç›¸å¯¹è¯¯å·®: {avg_rel_error:.4e}")


def main():
    parser = argparse.ArgumentParser(
        description="PDEBench æ€§èƒ½è¯„ä¼°è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--output", "-o",
        help="ä¿å­˜è¯¦ç»†æŠ¥å‘Šçš„ JSON æ–‡ä»¶è·¯å¾„",
        default=None
    )
    parser.add_argument(
        "--keep-artifacts",
        action="store_true",
        help="ä¿ç•™ artifacts_bench ç›®å½•ï¼ˆä¸æ¸…ç†æ—§æ•°æ®ï¼‰"
    )
    parser.add_argument(
        "--log-history",
        action="store_true",
        help="å°†ç»“æœè¿½åŠ åˆ° experiment_history.jsonl ç”¨äºåç»­ç»˜å›¾åˆ†æ"
    )
    parser.add_argument(
        "--experiment-id",
        help="å®éªŒæ ‡è¯†ç¬¦ï¼ˆç”¨äºåŒºåˆ†ä¸åŒçš„ä¼˜åŒ–å°è¯•ï¼‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³",
        default=None
    )
    
    args = parser.parse_args()
    
    try:
        exit_code = run_benchmark(
            keep_artifacts=args.keep_artifacts,
            output_file=args.output,
            log_history=args.log_history,
            experiment_id=args.experiment_id
        )
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nğŸ’¥ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(2)


if __name__ == "__main__":
    main()

